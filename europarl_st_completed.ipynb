{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоматическое определение языка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dronvol/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import unicodedata\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем решать задачу определения языка печатного текста. В файле `europarl.test.txt` содержатся записи дебатов в Европарламенте. Каждая строка содержит код языка и высказывание на этом языке, например, на болгарском:\n",
    "\n",
    "`bg\t(DE) Г-н председател, след повече от 300 години колониално управление и след като континентът се превърна в арена на Студената война, днес Латинска Америка вече е един от нововъзникващите региони в света.`\n",
    "\n",
    "Код языка будет целевой переменной, а из высказывания нам предстоит извлечь признаки.\n",
    "\n",
    "Один из возможных подходов состоит в том, чтобы в качестве признаков использовать тройки из подряд идущих символов, встречающихся в словах. Предположение состоит в том, что для каждого языка список наиболее популярных троек более-менее уникален. Попробуем проверить это предположение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS_PATH = \"./europarl.test.txt\" # Path to the data file\n",
    "N_GRAM = 3 # Extract symbol sequences of length N\n",
    "TOP_TOKENS = 10 # Number of top selected n-grams for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_documents(data_path):\n",
    "    \"\"\"\n",
    "    Reads a sequence of documents from the text file\n",
    "    located on a given path.\n",
    "\n",
    "    Returns:\n",
    "        A generator of tuples (LANG_CODE, unicode)\n",
    "    \"\"\"\n",
    "    with codecs.open(data_path, 'rU', \"utf-8\") as data_file:\n",
    "        for line in data_file:\n",
    "            lang, doc = line.strip().split('\\t')\n",
    "            yield lang, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_document(doc):\n",
    "    \"\"\"\n",
    "    Convert document to lower-case and remove accents\n",
    "    \n",
    "    TODO: Implement this\n",
    "\n",
    "    Returns:\n",
    "        A normalised document as unicode\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', doc.lower()) if not unicodedata.combining(c))#u\"this is example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_document(doc, n):\n",
    "    \"\"\"\n",
    "    Split document in N-Grams\n",
    "    \n",
    "    TODO: implement this\n",
    "\n",
    "    Returns:\n",
    "        Iterable (generator or list) of unicode n-grams\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.WordPunctTokenizer()\n",
    "    for token in tokenizer.tokenize(doc):\n",
    "        if len(token) >=n:\n",
    "            for ngram in nltk.ngrams(token, n):\n",
    "                yield u\"\".join(ngram)\n",
    "    #return [u\"thi\", u\"his\", u\"exa\", u\"xam\", u\"amp\", u\"mpl\", u\"ple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом нам необходимо зачитать данные из файла. Будем читать 3 структуры данных:\n",
    "\n",
    "- docs - список словарей, каждый из которых соответствует одному документу и содержит количество вхождений для каждой n-граммы (токена)\n",
    "- langs - список, содержащий классы докуметов (каждому коду языка соответствует числовой класс)\n",
    "- lang_freq - словарь, который нужен для подсчета ниболее популярных n-грам для каждого языка. Элементы этого словаря: код языка -> (id класса, частоты n-грам (аналогично docs)) \n",
    "\n",
    "Для того, чтобы заработал код, зачитывающий данные, необходимо (до) реализовать функции, перечисленные выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found language bg: 0\n",
      "Found language cs: 1\n",
      "Found language da: 2\n",
      "Found language de: 3\n",
      "Found language el: 4\n",
      "Found language en: 5\n",
      "Found language es: 6\n",
      "Found language et: 7\n",
      "Found language fi: 8\n",
      "Found language fr: 9\n",
      "Found language hu: 10\n",
      "Found language it: 11\n",
      "Found language lt: 12\n",
      "Found language lv: 13\n",
      "Found language nl: 14\n",
      "Found language pl: 15\n",
      "Found language pt: 16\n",
      "Found language ro: 17\n",
      "Found language sk: 18\n",
      "Found language sl: 19\n",
      "Found language sv: 20\n"
     ]
    }
   ],
   "source": [
    "# A list of dicts, each representing one document in format:\n",
    "# {token: count1, ...}\n",
    "docs = []\n",
    "# Language code for each dict (0-based)\n",
    "langs = []\n",
    "# A list of tuples, each tuple corresponds to one language\n",
    "# First compunent is the code of the language, second is its token frequencies\n",
    "# Contains entries like {lang_code: (lang_id, {token_frequencies})}\n",
    "lang_freq = {}\n",
    "\n",
    "for lang, doc in read_documents(DS_PATH):\n",
    "    normalized_doc = normalise_document(doc)\n",
    "\n",
    "    token_freq = {}\n",
    "    for token in tokenize_document(normalized_doc, N_GRAM):\n",
    "        token_freq[token] = 1 + token_freq.get(token, 0)\n",
    "        if lang not in lang_freq:\n",
    "            print \"Found language %s: %d\" % (lang, len(lang_freq))\n",
    "            lang_freq[lang] = (len(lang_freq), {})\n",
    "        lang_freq[lang][1][token] = 1 + lang_freq[lang][1].get(token, 0)\n",
    "\n",
    "    docs.append(token_freq)\n",
    "    langs.append(lang_freq[lang][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "Здесь предстоит выбрать топовые n-граммы для каждого языка (`select_features`) и отфильтровать документы так, чтобы в них остались только отобранные (`keep_only_features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_features(lang_freq, top_tokens):\n",
    "    \"\"\"\n",
    "    From each language selects top_tokens to be used as features\n",
    "    \n",
    "    TODO: Implement this\n",
    "\n",
    "    Returns:\n",
    "        set(unicode tokens)\n",
    "    \"\"\"\n",
    "    features = set()\n",
    "    for lang, (lid, token_freq) in lang_freq.iteritems():\n",
    "        sorted_token_freq = sorted(token_freq.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "        for token, freq in sorted_token_freq[:top_tokens]:\n",
    "            features.add(token)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_only_features(docs, features):\n",
    "    \"\"\"\n",
    "    Removes non-feature tokens from the document representations\n",
    "    \"\"\"\n",
    "    for token_freq in docs:\n",
    "        for token in token_freq.keys():\n",
    "            if token not in features:\n",
    "                del token_freq[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape x=(21000 x 151) y=21000\n"
     ]
    }
   ],
   "source": [
    "# Select top n features for each lang\n",
    "features = select_features(lang_freq, TOP_TOKENS)\n",
    "# Remove from documents all features except the selected\n",
    "keep_only_features(docs, features)\n",
    "\n",
    "# Transform documents to numpy matrix\n",
    "dv = DictVectorizer()\n",
    "x = dv.fit_transform(docs).todense()\n",
    "y = numpy.array(langs)\n",
    "print \"Data set shape x=(%d x %d) y=%d\" % (x.shape[0], x.shape[1], len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и настройка модели\n",
    "\n",
    "В этом пункте требуется инициализировать модель (NB) и перебрать различные значения параметров. Предлагается попробовать Multinumial и Bernoulli варианты из sklearn и несколько значений параметров (alpha, binarize, fit_prior) и выбрать наилучшую модель на основании метрики `accuracy`, полученной на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Initialise an NB model, supported by Sklearn\n",
    "\n",
    "    Returns:\n",
    "        Sklearn model instance\n",
    "    \"\"\"\n",
    "    return MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(model, x, y, folds=10):\n",
    "    \"\"\"\n",
    "    Computes cross-validation score for the given data set and model.\n",
    "    \n",
    "    TODO: Implement this\n",
    "\n",
    "    Returns:\n",
    "        A numpy.array of accuracy scores.\n",
    "    \"\"\"\n",
    "    sorces = cross_val_score(model, x, y, cv=folds)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(model, x, y, class_ind=0):    \n",
    "    # Compute ROC curve\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5, random_state=0)\n",
    "    fit = model.fit(x_train, y_train)\n",
    "    y_prob = fit.predict_proba(x_test)    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, class_ind], pos_label=class_ind)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    plt.fill_between(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for class index %s' % class_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fae78729a3f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Print cross-validated accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Model mean accuracy: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-cf844720d5ac>\u001b[0m in \u001b[0;36mvalidate_model\u001b[1;34m(model, x, y, folds)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[0msorces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: global name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "# Print cross-validated accuracy\n",
    "scores = validate_model(model, x, y)\n",
    "print \"Model mean accuracy: {}\".format(numpy.mean(scores))\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_roc(model, x, y, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Дополнительно\n",
    "\n",
    "- Выясните, какие классы чаще всего путаются (на кросс-валидации!)\n",
    "- Сравните NB и KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
